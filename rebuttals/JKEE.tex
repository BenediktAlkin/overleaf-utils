Thank you for your thorough review and helpful comments which were very useful to improve the paper. We address your points individually.


**Why UPT is a better scalable latent space model**

Latent Spectral Model uses geo-FNO (which one could consider as predecessor of GINO) to handle irregular grids. Therefore, it is somewhat similar to GINO as it has to map its input into a fixed regular grid representation.

LE-PDE only considers small-scale regular grid data and their design requires train and test data to have the *exact* same resolution (as mentioned in Appendix C of their paper). Therefore, its application to irregular grid data is impractical as a lot of data has different number of particles/mesh cells during test time. UPT can handle arbitrary input resolutions during training and generalizes well to higher input resolutions (as shown in Figure 5).

FactFormer does not compress its input into a smaller latent space and therefore doesn't scale to large input sizes. Their method proposes an efficient attention mechanism, but their compute requirement would be similar to linear transformers and therefore become infeasible on large systems (as shown in Figure 2).

**Comparison to transformer based models on regular grid**

We compare UPT on two regular grid benchmarks as shown in the general response and in the supplemental pdf Tables 1 to 3. UPT outperforms all compared methods (such as OFormer or DPOT) -- often by quite a margin -- without being specifically designed for regular grid datasets.



**Justification why UPTS are neural operators**

Thank you for bringing up this important point. To avoid overloading the notation, we will provide a brief sketch of how universality can be established for our architecture. For transformer-based neural operators, universal approximation has been recently demonstrated in [1], Section 5. The arguments in this work are heavily based on [2], which establishes that nonlinearity and nonlocality are crucial for universality. By demonstrating that the attention mechanism can, under appropriate weight choices, function as an averaging operator, the results from [2] are directly applicable. For detailed proof, refer to Theorem 22 in [1].
Our algorithm fits within this framework as well: we employ nonlinear, attention-based encoders and decoders (as allowed by the results of [2], Section 2) and utilize attention layers in the latent space.

Please let us know if you require further explanations. We will provide detailed information in an updated version of the manuscript.



[1] Calvello et al., "Continuum Attention for Neural Operators", arXiv 2024, https://arxiv.org/abs/2406.06486

[2] Lanthaler et al., "The nonlocal neural operator: Universal approximation" arXiv 2024, https://arxiv.org/abs/2304.13221


**Rollout stability**

Our training procedure doesn't use any special methods to stabilize the rollout because this typically comes at increased training costs. However, one could easily apply such techniques to UPT.
As we found our models to produce fairly stable rollouts even without additional techniques, we leave this direction for future work.





**Thoughts on latent space vs physics space**

We see the latent rollout as a promising way to speedup inference, particularly for large-scale systems where encoding and decoding takes up most of the runtime.
UPT offers the flexibility to invest resources into training it as a latent rollout model (by using the inverse encoding/decoding objectives) or to save resources during training (by omitting the reconstruction objectives) at the cost of increased inference time. We consider this a valuable tool to have for neural operators.

Additionally, the latent rollout enables applicability to Lagrangian simulations. As UPT models the underlying field instead of tracking individual particle positions it does not have access to particle locations at inference time. Therefore, autoregressive rollouts are impossible since the encoder requires particle positions as input. Using the latent rollout, it is sufficient to encode the initial particle positions into the latent space, which can then be propagated without the knowledge of any spatial positions. After propagating the latent space forward in time, one can simply query the latent space at arbitrary positions to evaluate the underlying field at given positions. We showcase this in Figure 7 where the latent space is queried with regular grid coordinates (white arrows).



**Limitations of non-regular gridded data**


We are interested in complex physics simulations which are often simulated using e.g., finite element meshes. Several phenomena are modeled by particle-based simulations such as smoothed particle hydrodynamics, material point methods, or discrete element methods. Many phenomena even require a coupling of different aforementioned simulation types. This is very much driven by daily life engineering applications, and as such was a main motivation for developing UPT.





**Limitations on fixed latent space**


In the current architecture of UPT, we consider a fixed latent space as it has proven to be an efficient way to compress the input into a fixed size representation to enable scaling to large-scale systems while remaining compute efficient.
However, if an application requires a variable sized latent space, one could also remove the perceiver pooling layer in the encoder. With this change the number of supernodes is equal to the number of latent tokens and complex problems could be tackled by a larger supernode count. While we currently do not consider a setting where this is necessary, we show that the performance of UPT steadily increases with the number of supernodes during training (Figure 9 in Appendix C.4.3).
Additionally, UPT's performance improves when more supernodes are used during evaluation than were utilized during training (Figure 8 in Appendix C.4.2).