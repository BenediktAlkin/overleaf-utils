We thank all reviewers for their positive feedback and for their constructive comments and suggestions.
We are pleased to see that the reviewers highlighted the clarity of our paper and appreciated our detailed and thorough experiments. Several reviewers recognized the efficiency of our approach by encoding and decoding into a latent space as well as UPTs ability to handle both Lagrangian and Eulerian data.



To address the questions of the reviewers, we included the following new experiments. Please find visualizations thereof in the supplemental rebuttal pdf.

**Experiments on a regular grid Navier-Stokes equations dataset**

We run comparisons against different Transformer baselines on regular gridded Navier-Stokes equations data (data, baseline results and evaluation protocol taken from [1]). UPT outperforms all methods which are specifically designed for regularly gridded data.

As baseline transformers often train small models, we first compare on a small scale, where UPT significantly outperforms transformer baselines.

| Model | # Params | Rel. L2 Error  |
|---|---|---|
| FNO | 0.5M | 9.12 \%  |
| FFNO | 1.3M | 8.39 \%  |
| GK-T |  1.6M | 9.52 \%  |
| GNOT |  1.8M | 17.20 \%  |
| Oformer | 1.9M |  13.50 \%  |
| UPT-T | 1.8M | **5.08** \%  |

We also compare on larger scales, where UPT also outperforms competititors, even if they train much larger models or pre-train (PT) on more data followed by fine-tuning (FT) on the Navier Stokes dataset.


| Model | # Params | Rel. L2 Error  |
|---|---|---|
| DPOT-Ti |  7M | 12.50 \%  |
| DPOT-S | 30M |  9.91 \%  |
| DPOT-L (PT) | 500M | 7.98 \%  |
| DPOT-L (FT) | 500M | 2.78 \%  |
| DPOT-H (PT) | 1.03B | 3.79 \%  |
| CViT-S | 13M | 3.75 \%  |
| CViT-B | 30M | 3.18 \%  |
| %CViT-L/4 | 92M | 2.35 \%  |
| UPT-S | 13M | 3.12 \%  |
| UPT-B | 30M | **2.69** \%  |
| %UPT-L/8 | 87M | 2.35 \%  |


[1] Hao et al., "DPOT: Auto-Regressive Denoising Operator Transformer for Large-Scale PDE Pre-Training", ICML 2024, https://arxiv.org/abs/2403.03542



**Experiments on a regular grid Shallow Water equations dataset**


We run comparisons against UNet, FNO, Dilated ResNet variants on regular gridded Shallow Water eqations data (data, baseline results and evaluation protocol taken from [2]). Similarly to the Navier-Stokes experiments, UPT can outperform methods which are specifically designed for regularly gridded data.


| Model | # Params | Rel. L2 Error  |
|---|---|---|
| DilResNet | 4.2M | 13.20 \%  |
| U-Net | 148M | 5.68 \%  |
| FNO | 268M | 3.97 \%  |
| %U-F2Net | 344M | 1.89 \%  |
| %UNO | 44M | 3.79 \%  |
| CViT-S | 13M | 4.47 \%  |
| %CViT-B | 30M | 2.69 \%  |
| %CViT-L/4 | 92M | 1.56 \%  |
| %UPT-T | 1.8M | 11.75 \%  |
| UPT-S | 13M | **3.96** \%  |
| %UPT-B | 30M | 3.30 \%  |
| %UPT-L/8 | 87M |  \%  |

[2] Wang et al., "Bridging Operator Learning and Neural Fields:. A Unifying Perspective", arXiv 2024, https://arxiv.org/abs/2405.13998

**Impact of different positional encodings**

We added ablation results with different positional encoding.
Using a Fourier feature mapping from [3] resulted in an slightly better performance in the TGV2D experiments. We visualize this in Figure 1 of the supplemental rebuttal pdf.

[3] Tancik et al., "Fourier features let networks learn high frequency functions in low dimensional domains", NeurIPS 2020, https://arxiv.org/abs/2006.10739


**UPT scaling clarifications**

We added experiments to clarify the scaling of UPTs in the transient flow experiments by training small UPT models that perform similar to larger GINO models. We visualize the results in Figure 2 of the supplemental rebuttal pdf, which shows that UPT scales well when increasing parameter counts. This experiment should clarify that UPTs do scale well with parameter counts, but as the testloss of UPT (in Figure 5 of the paper) is significantly lower than for GINO, its much harder to improve it further.


**Further additions to the paper**

We followed the reviewers comments and suggestions, e.g., by including pseudocode of an UPT forward pass, detailed paragraphs on positional encoding and conditioning mechanisms, implementation details of the supernode pooling/radius graph and additional related works.


In addition, we followed the reviewers comments and suggestions, e.g. by discussing their raised points in the Discussion section or added clarifications, contextualization, and references.